{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61f56afc-bc15-46a4-8eb1-d940c332cf52",
   "metadata": {},
   "source": [
    "# Tokenizers\n",
    "\n",
    "Please can I bring you back to the wonderful Google Colab where we'll look at different Tokenizers:\n",
    "\n",
    "https://colab.research.google.com/drive/1WD6Y2N7ctQi1X9wa6rpkg8UfyA4iSVuz?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84089f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement torch==2.5.1+cu124 (from versions: 2.6.0+cu124)\n",
      "ERROR: No matching distribution found for torch==2.5.1+cu124\n"
     ]
    }
   ],
   "source": [
    "# if this gives an \"ERROR\" about pip dependency conflicts, ignore it! It doesn't affect anything.\n",
    "\n",
    "!pip install -q --upgrade torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1+cu124 --index-url https://download.pytorch.org/whl/cu124\n",
    "\n",
    "!pip install -q --upgrade transformers==4.48.3 datasets==3.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cbac7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoTokenizer\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e270b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()  # loads environment variables from a .env file if present\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3754cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a public model to avoid gated repo errors\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efc2b6f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101,\n",
       " 1045,\n",
       " 2572,\n",
       " 7568,\n",
       " 2000,\n",
       " 2265,\n",
       " 19204,\n",
       " 17629,\n",
       " 2015,\n",
       " 1999,\n",
       " 2895,\n",
       " 2000,\n",
       " 2026,\n",
       " 2222,\n",
       " 2213,\n",
       " 6145,\n",
       " 102]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
    "tokens = tokenizer.encode(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faed56d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "069132d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] i am excited to show tokenizers in action to my llm engineers [SEP]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "842ea56d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'i',\n",
       " 'am',\n",
       " 'excited',\n",
       " 'to',\n",
       " 'show',\n",
       " 'token',\n",
       " '##izer',\n",
       " '##s',\n",
       " 'in',\n",
       " 'action',\n",
       " 'to',\n",
       " 'my',\n",
       " 'll',\n",
       " '##m',\n",
       " 'engineers',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9307db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[PAD]': 0, '[UNK]': 100, '[CLS]': 101, '[SEP]': 102, '[MASK]': 103}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.vocab\n",
    "tokenizer.get_added_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40aa0d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the existing public tokenizer: distilbert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "# The requested model is gated and cannot be accessed without permission.\n",
    "# Use the already loaded public tokenizer instead.\n",
    "# tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct', trust_remote_code=True)\n",
    "print(\"Using the existing public tokenizer: distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e0385b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the existing public tokenizer: distilbert-base-uncased\n",
      "[101, 2425, 1037, 2422, 1011, 18627, 8257, 2005, 1037, 2282, 1997, 2951, 6529, 102]\n"
     ]
    }
   ],
   "source": [
    "# Use the already loaded public tokenizer instead of a gated model\n",
    "print(\"Using the existing public tokenizer: distilbert-base-uncased\")\n",
    "\n",
    "# Example: encode the messages using the available tokenizer\n",
    "prompt = tokenizer.encode(messages[1]['content'])\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a38a86aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHI3_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "QWEN2_MODEL_NAME = \"Qwen/Qwen2-7B-Instruct\"\n",
    "STARCODER2_MODEL_NAME = \"bigcode/starcoder2-3b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40273bb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lenovo\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-4k-instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 2572, 7568, 2000, 2265, 19204, 17629, 2015, 1999, 2895, 2000, 2026, 2222, 2213, 6145, 102]\n",
      "\n",
      "['I', 'am', 'excited', 'to', 'show', 'Token', 'izers', 'in', 'action', 'to', 'my', 'L', 'LM', 'engine', 'ers']\n"
     ]
    }
   ],
   "source": [
    "phi3_tokenizer = AutoTokenizer.from_pretrained(PHI3_MODEL_NAME)\n",
    "\n",
    "text = \"I am excited to show Tokenizers in action to my LLM engineers\"\n",
    "print(tokenizer.encode(text))\n",
    "print()\n",
    "tokens = phi3_tokenizer.encode(text)\n",
    "print(phi3_tokenizer.batch_decode(tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cdbe78b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer does not support chat templates.\n",
      "\n",
      "<|system|>\n",
      "You are a helpful assistant<|end|>\n",
      "<|user|>\n",
      "Tell a light-hearted joke for a room of Data Scientists<|end|>\n",
      "<|assistant|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Only call apply_chat_template if the tokenizer supports it\n",
    "if getattr(tokenizer, \"chat_template\", None):\n",
    "\tprint(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "else:\n",
    "\tprint(\"tokenizer does not support chat templates.\")\n",
    "\n",
    "print()\n",
    "\n",
    "if getattr(phi3_tokenizer, \"chat_template\", None):\n",
    "\tprint(phi3_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "else:\n",
    "\tprint(\"phi3_tokenizer does not support chat templates.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d0eede3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizer does not support chat templates.\n",
      "\n",
      "<|system|>\n",
      "You are a helpful assistant<|end|>\n",
      "<|user|>\n",
      "Tell a light-hearted joke for a room of Data Scientists<|end|>\n",
      "<|assistant|>\n",
      "\n",
      "\n",
      "qwen2_tokenizer does not support chat templates or is not defined.\n"
     ]
    }
   ],
   "source": [
    "if getattr(tokenizer, \"chat_template\", None):\n",
    "\tprint(tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "else:\n",
    "\tprint(\"tokenizer does not support chat templates.\")\n",
    "\n",
    "print()\n",
    "\n",
    "if getattr(phi3_tokenizer, \"chat_template\", None):\n",
    "\tprint(phi3_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "else:\n",
    "\tprint(\"phi3_tokenizer does not support chat templates.\")\n",
    "\n",
    "print()\n",
    "\n",
    "if 'qwen2_tokenizer' in locals() and getattr(qwen2_tokenizer, \"chat_template\", None):\n",
    "\tprint(qwen2_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True))\n",
    "else:\n",
    "\tprint(\"qwen2_tokenizer does not support chat templates or is not defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "48d052f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Lenovo\\.cache\\huggingface\\hub\\models--bigcode--starcoder2-3b. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222=\n",
      "\n",
      "610=def\n",
      "17966= hello\n",
      "100=_\n",
      "5879=world\n",
      "45=(\n",
      "6427=person\n",
      "731=):\n",
      "353=\n",
      " \n",
      "1489= print\n",
      "459=(\"\n",
      "8302=Hello\n",
      "411=\",\n",
      "4944= person\n",
      "46=)\n",
      "222=\n",
      "\n"
     ]
    }
   ],
   "source": [
    "starcoder2_tokenizer = AutoTokenizer.from_pretrained(STARCODER2_MODEL_NAME, trust_remote_code=True)\n",
    "code = \"\"\"\n",
    "def hello_world(person):\n",
    "  print(\"Hello\", person)\n",
    "\"\"\"\n",
    "tokens = starcoder2_tokenizer.encode(code)\n",
    "for token in tokens:\n",
    "  print(f\"{token}={starcoder2_tokenizer.decode(token)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b596ca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add \"C:\\Users\\Lenovo\\Desktop\\pipline-s-magic\\OpenSource-Gen-AI\\Tokenizers.ipynb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94e65647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main b809845] added tokenizers\n",
      " 1 file changed, 37 insertions(+)\n",
      " create mode 100644 OpenSource-Gen-AI/Tokenizers.ipynb\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"added tokenizers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59fe3d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "branch 'main' set up to track 'origin/main'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://github.com/SUSH9391/pipline-s-magic.git\n",
      "   3bd45ee..b809845  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git push -u origin main"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
